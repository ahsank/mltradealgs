{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iRWFgIbnzajB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q yahoo_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ahsank/miniconda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0MuPGKaH3W59"
   },
   "outputs": [],
   "source": [
    "!mkdir -p data results log\n",
    "! cp ~/Google\\ Drive/My\\ Drive/colab/results/* ./results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4AuykWZdTyh"
   },
   "source": [
    "Todos:\n",
    "\n",
    " \n",
    "\n",
    "1.   Load previously saved model before training\n",
    "2.   Allow eval only without test/train spliting\n",
    "3.   Incremental data load\n",
    "4.    Add S&P, QQQ etc to the model\n",
    "5. Save model to google drive\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fqx81EuXwsnI"
   },
   "outputs": [],
   "source": [
    "from runml import pipeline,findata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'runml.findata' from '/Users/ahsank/src/runml/runml/findata.py'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(pipeline)\n",
    "reload(findata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# profit factor 2 = 50% of predicted gain due to early profit taking\n",
    "def addAlloc(df, stop_loss, profit_factor=2):\n",
    "  df['Alloc'] = df['Accuracy']/stop_loss - (1-df['Accuracy'])*profit_factor/abs(df['Gain'])\n",
    "  df['Alloc'] = np.where(df['Alloc'] < 0, 0, df['Alloc'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRNq5x6mTG-q",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0050 - mean_absolute_error: 0.0726\n",
      "Epoch 1: val_loss improved from inf to 0.00412, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 25s 32ms/step - loss: 0.0050 - mean_absolute_error: 0.0726 - val_loss: 0.0041 - val_mean_absolute_error: 0.0660\n",
      "Epoch 2/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0043 - mean_absolute_error: 0.0677\n",
      "Epoch 2: val_loss did not improve from 0.00412\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0043 - mean_absolute_error: 0.0677 - val_loss: 0.0043 - val_mean_absolute_error: 0.0654\n",
      "Epoch 3/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0649\n",
      "Epoch 3: val_loss improved from 0.00412 to 0.00375, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0040 - mean_absolute_error: 0.0649 - val_loss: 0.0037 - val_mean_absolute_error: 0.0622\n",
      "Epoch 4/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0636\n",
      "Epoch 4: val_loss improved from 0.00375 to 0.00361, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0038 - mean_absolute_error: 0.0636 - val_loss: 0.0036 - val_mean_absolute_error: 0.0632\n",
      "Epoch 5/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0625\n",
      "Epoch 5: val_loss improved from 0.00361 to 0.00348, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0037 - mean_absolute_error: 0.0625 - val_loss: 0.0035 - val_mean_absolute_error: 0.0597\n",
      "Epoch 6/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0618\n",
      "Epoch 6: val_loss did not improve from 0.00348\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0036 - mean_absolute_error: 0.0618 - val_loss: 0.0037 - val_mean_absolute_error: 0.0607\n",
      "Epoch 7/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0610\n",
      "Epoch 7: val_loss did not improve from 0.00348\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0036 - mean_absolute_error: 0.0610 - val_loss: 0.0037 - val_mean_absolute_error: 0.0608\n",
      "Epoch 8/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0602\n",
      "Epoch 8: val_loss did not improve from 0.00348\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0035 - mean_absolute_error: 0.0602 - val_loss: 0.0039 - val_mean_absolute_error: 0.0626\n",
      "Epoch 9/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0597\n",
      "Epoch 9: val_loss improved from 0.00348 to 0.00331, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0034 - mean_absolute_error: 0.0597 - val_loss: 0.0033 - val_mean_absolute_error: 0.0584\n",
      "Epoch 10/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0592\n",
      "Epoch 10: val_loss did not improve from 0.00331\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0034 - mean_absolute_error: 0.0592 - val_loss: 0.0034 - val_mean_absolute_error: 0.0588\n",
      "Epoch 11/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0585\n",
      "Epoch 11: val_loss improved from 0.00331 to 0.00325, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0033 - mean_absolute_error: 0.0585 - val_loss: 0.0032 - val_mean_absolute_error: 0.0582\n",
      "Epoch 12/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0581\n",
      "Epoch 12: val_loss did not improve from 0.00325\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0032 - mean_absolute_error: 0.0581 - val_loss: 0.0033 - val_mean_absolute_error: 0.0586\n",
      "Epoch 13/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0574\n",
      "Epoch 13: val_loss did not improve from 0.00325\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0032 - mean_absolute_error: 0.0574 - val_loss: 0.0038 - val_mean_absolute_error: 0.0625\n",
      "Epoch 14/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0031 - mean_absolute_error: 0.0568\n",
      "Epoch 14: val_loss improved from 0.00325 to 0.00314, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0031 - mean_absolute_error: 0.0568 - val_loss: 0.0031 - val_mean_absolute_error: 0.0576\n",
      "Epoch 15/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 0.0030 - mean_absolute_error: 0.0562\n",
      "Epoch 15: val_loss did not improve from 0.00314\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0030 - mean_absolute_error: 0.0562 - val_loss: 0.0032 - val_mean_absolute_error: 0.0576\n",
      "Epoch 16/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0030 - mean_absolute_error: 0.0556\n",
      "Epoch 16: val_loss improved from 0.00314 to 0.00296, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0030 - mean_absolute_error: 0.0556 - val_loss: 0.0030 - val_mean_absolute_error: 0.0563\n",
      "Epoch 17/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0029 - mean_absolute_error: 0.0549\n",
      "Epoch 17: val_loss improved from 0.00296 to 0.00291, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0029 - mean_absolute_error: 0.0549 - val_loss: 0.0029 - val_mean_absolute_error: 0.0545\n",
      "Epoch 18/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0028 - mean_absolute_error: 0.0544\n",
      "Epoch 18: val_loss did not improve from 0.00291\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0028 - mean_absolute_error: 0.0544 - val_loss: 0.0029 - val_mean_absolute_error: 0.0552\n",
      "Epoch 19/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0028 - mean_absolute_error: 0.0538\n",
      "Epoch 19: val_loss improved from 0.00291 to 0.00277, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0028 - mean_absolute_error: 0.0538 - val_loss: 0.0028 - val_mean_absolute_error: 0.0540\n",
      "Epoch 20/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0528\n",
      "Epoch 20: val_loss did not improve from 0.00277\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0027 - mean_absolute_error: 0.0528 - val_loss: 0.0029 - val_mean_absolute_error: 0.0550\n",
      "Epoch 21/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0026 - mean_absolute_error: 0.0522\n",
      "Epoch 21: val_loss improved from 0.00277 to 0.00271, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0026 - mean_absolute_error: 0.0522 - val_loss: 0.0027 - val_mean_absolute_error: 0.0533\n",
      "Epoch 22/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0515\n",
      "Epoch 22: val_loss improved from 0.00271 to 0.00256, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0025 - mean_absolute_error: 0.0515 - val_loss: 0.0026 - val_mean_absolute_error: 0.0512\n",
      "Epoch 23/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0504\n",
      "Epoch 23: val_loss improved from 0.00256 to 0.00254, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0024 - mean_absolute_error: 0.0504 - val_loss: 0.0025 - val_mean_absolute_error: 0.0518\n",
      "Epoch 24/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0495\n",
      "Epoch 24: val_loss did not improve from 0.00254\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0026 - val_mean_absolute_error: 0.0521\n",
      "Epoch 25/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0482\n",
      "Epoch 25: val_loss improved from 0.00254 to 0.00238, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0022 - mean_absolute_error: 0.0482 - val_loss: 0.0024 - val_mean_absolute_error: 0.0503\n",
      "Epoch 26/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0474\n",
      "Epoch 26: val_loss improved from 0.00238 to 0.00236, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0021 - mean_absolute_error: 0.0474 - val_loss: 0.0024 - val_mean_absolute_error: 0.0514\n",
      "Epoch 27/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0463\n",
      "Epoch 27: val_loss improved from 0.00236 to 0.00202, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0020 - mean_absolute_error: 0.0463 - val_loss: 0.0020 - val_mean_absolute_error: 0.0479\n",
      "Epoch 28/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0018 - mean_absolute_error: 0.0450\n",
      "Epoch 28: val_loss improved from 0.00202 to 0.00178, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 0.0018 - mean_absolute_error: 0.0450 - val_loss: 0.0018 - val_mean_absolute_error: 0.0443\n",
      "Epoch 29/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0018 - mean_absolute_error: 0.0439\n",
      "Epoch 29: val_loss did not improve from 0.00178\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0018 - mean_absolute_error: 0.0439 - val_loss: 0.0018 - val_mean_absolute_error: 0.0449\n",
      "Epoch 30/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0424\n",
      "Epoch 30: val_loss improved from 0.00178 to 0.00174, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0016 - mean_absolute_error: 0.0424 - val_loss: 0.0017 - val_mean_absolute_error: 0.0442\n",
      "Epoch 31/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0414\n",
      "Epoch 31: val_loss improved from 0.00174 to 0.00152, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0015 - mean_absolute_error: 0.0414 - val_loss: 0.0015 - val_mean_absolute_error: 0.0417\n",
      "Epoch 32/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0400\n",
      "Epoch 32: val_loss did not improve from 0.00152\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 0.0014 - mean_absolute_error: 0.0400 - val_loss: 0.0017 - val_mean_absolute_error: 0.0438\n",
      "Epoch 33/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0389\n",
      "Epoch 33: val_loss did not improve from 0.00152\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0014 - mean_absolute_error: 0.0389 - val_loss: 0.0017 - val_mean_absolute_error: 0.0434\n",
      "Epoch 34/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0373\n",
      "Epoch 34: val_loss did not improve from 0.00152\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0012 - mean_absolute_error: 0.0373 - val_loss: 0.0015 - val_mean_absolute_error: 0.0424\n",
      "Epoch 35/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0363\n",
      "Epoch 35: val_loss improved from 0.00152 to 0.00132, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 0.0012 - mean_absolute_error: 0.0363 - val_loss: 0.0013 - val_mean_absolute_error: 0.0383\n",
      "Epoch 36/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0348\n",
      "Epoch 36: val_loss improved from 0.00132 to 0.00118, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 0.0011 - mean_absolute_error: 0.0348 - val_loss: 0.0012 - val_mean_absolute_error: 0.0365\n",
      "Epoch 37/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0338\n",
      "Epoch 37: val_loss did not improve from 0.00118\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 0.0010 - mean_absolute_error: 0.0338 - val_loss: 0.0012 - val_mean_absolute_error: 0.0374\n",
      "Epoch 38/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 9.3899e-04 - mean_absolute_error: 0.0327\n",
      "Epoch 38: val_loss improved from 0.00118 to 0.00112, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 9.3900e-04 - mean_absolute_error: 0.0327 - val_loss: 0.0011 - val_mean_absolute_error: 0.0357\n",
      "Epoch 39/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 9.2495e-04 - mean_absolute_error: 0.0323\n",
      "Epoch 39: val_loss did not improve from 0.00112\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 9.2550e-04 - mean_absolute_error: 0.0323 - val_loss: 0.0012 - val_mean_absolute_error: 0.0377\n",
      "Epoch 40/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 8.2229e-04 - mean_absolute_error: 0.0307\n",
      "Epoch 40: val_loss improved from 0.00112 to 0.00105, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 8.2227e-04 - mean_absolute_error: 0.0307 - val_loss: 0.0011 - val_mean_absolute_error: 0.0352\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727/728 [============================>.] - ETA: 0s - loss: 8.0021e-04 - mean_absolute_error: 0.0302\n",
      "Epoch 41: val_loss improved from 0.00105 to 0.00104, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 8.0056e-04 - mean_absolute_error: 0.0303 - val_loss: 0.0010 - val_mean_absolute_error: 0.0348\n",
      "Epoch 42/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 7.6704e-04 - mean_absolute_error: 0.0297\n",
      "Epoch 42: val_loss improved from 0.00104 to 0.00091, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 7.6656e-04 - mean_absolute_error: 0.0297 - val_loss: 9.1165e-04 - val_mean_absolute_error: 0.0327\n",
      "Epoch 43/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 6.9947e-04 - mean_absolute_error: 0.0283\n",
      "Epoch 43: val_loss improved from 0.00091 to 0.00090, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 6.9933e-04 - mean_absolute_error: 0.0283 - val_loss: 9.0206e-04 - val_mean_absolute_error: 0.0321\n",
      "Epoch 44/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 6.9327e-04 - mean_absolute_error: 0.0282\n",
      "Epoch 44: val_loss did not improve from 0.00090\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 6.9343e-04 - mean_absolute_error: 0.0282 - val_loss: 9.8036e-04 - val_mean_absolute_error: 0.0335\n",
      "Epoch 45/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 6.5551e-04 - mean_absolute_error: 0.0274\n",
      "Epoch 45: val_loss did not improve from 0.00090\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 6.5551e-04 - mean_absolute_error: 0.0274 - val_loss: 9.5714e-04 - val_mean_absolute_error: 0.0337\n",
      "Epoch 46/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 6.1378e-04 - mean_absolute_error: 0.0266\n",
      "Epoch 46: val_loss improved from 0.00090 to 0.00079, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 6.1378e-04 - mean_absolute_error: 0.0266 - val_loss: 7.8615e-04 - val_mean_absolute_error: 0.0302\n",
      "Epoch 47/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 6.0709e-04 - mean_absolute_error: 0.0265\n",
      "Epoch 47: val_loss improved from 0.00079 to 0.00073, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 6.0725e-04 - mean_absolute_error: 0.0265 - val_loss: 7.3103e-04 - val_mean_absolute_error: 0.0290\n",
      "Epoch 48/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 5.4587e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 48: val_loss improved from 0.00073 to 0.00072, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 5.4587e-04 - mean_absolute_error: 0.0251 - val_loss: 7.1737e-04 - val_mean_absolute_error: 0.0286\n",
      "Epoch 49/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 5.6089e-04 - mean_absolute_error: 0.0253\n",
      "Epoch 49: val_loss improved from 0.00072 to 0.00069, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 5.6089e-04 - mean_absolute_error: 0.0253 - val_loss: 6.8895e-04 - val_mean_absolute_error: 0.0281\n",
      "Epoch 50/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 5.0988e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 50: val_loss improved from 0.00069 to 0.00069, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 5.0988e-04 - mean_absolute_error: 0.0242 - val_loss: 6.8525e-04 - val_mean_absolute_error: 0.0286\n",
      "Epoch 51/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 4.9489e-04 - mean_absolute_error: 0.0238\n",
      "Epoch 51: val_loss did not improve from 0.00069\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 4.9465e-04 - mean_absolute_error: 0.0238 - val_loss: 7.6310e-04 - val_mean_absolute_error: 0.0296\n",
      "Epoch 52/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.7848e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 52: val_loss improved from 0.00069 to 0.00067, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 4.7848e-04 - mean_absolute_error: 0.0234 - val_loss: 6.6588e-04 - val_mean_absolute_error: 0.0277\n",
      "Epoch 53/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 4.7681e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 53: val_loss did not improve from 0.00067\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 4.7675e-04 - mean_absolute_error: 0.0234 - val_loss: 6.9455e-04 - val_mean_absolute_error: 0.0285\n",
      "Epoch 54/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.7122e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 54: val_loss improved from 0.00067 to 0.00065, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 4.7122e-04 - mean_absolute_error: 0.0231 - val_loss: 6.5080e-04 - val_mean_absolute_error: 0.0272\n",
      "Epoch 55/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 4.2142e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 55: val_loss improved from 0.00065 to 0.00064, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 4.2132e-04 - mean_absolute_error: 0.0219 - val_loss: 6.3859e-04 - val_mean_absolute_error: 0.0273\n",
      "Epoch 56/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 4.3604e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 56: val_loss improved from 0.00064 to 0.00063, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 4.3601e-04 - mean_absolute_error: 0.0221 - val_loss: 6.3253e-04 - val_mean_absolute_error: 0.0270\n",
      "Epoch 57/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.2246e-04 - mean_absolute_error: 0.0219\n",
      "Epoch 57: val_loss improved from 0.00063 to 0.00058, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 4.2246e-04 - mean_absolute_error: 0.0219 - val_loss: 5.7980e-04 - val_mean_absolute_error: 0.0254\n",
      "Epoch 58/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.0781e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 58: val_loss did not improve from 0.00058\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 4.0781e-04 - mean_absolute_error: 0.0216 - val_loss: 5.9358e-04 - val_mean_absolute_error: 0.0261\n",
      "Epoch 59/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 4.1002e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 59: val_loss did not improve from 0.00058\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 4.1014e-04 - mean_absolute_error: 0.0215 - val_loss: 7.0412e-04 - val_mean_absolute_error: 0.0294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.7361e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 60: val_loss improved from 0.00058 to 0.00058, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.7338e-04 - mean_absolute_error: 0.0206 - val_loss: 5.7599e-04 - val_mean_absolute_error: 0.0257\n",
      "Epoch 61/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.7391e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 61: val_loss improved from 0.00058 to 0.00049, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 3.7401e-04 - mean_absolute_error: 0.0206 - val_loss: 4.9335e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 62/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.5604e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 62: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 3.5604e-04 - mean_absolute_error: 0.0201 - val_loss: 5.4902e-04 - val_mean_absolute_error: 0.0247\n",
      "Epoch 63/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.4746e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 63: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.4752e-04 - mean_absolute_error: 0.0198 - val_loss: 6.4184e-04 - val_mean_absolute_error: 0.0271\n",
      "Epoch 64/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.5114e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 64: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 3.5114e-04 - mean_absolute_error: 0.0199 - val_loss: 5.5108e-04 - val_mean_absolute_error: 0.0244\n",
      "Epoch 65/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.4059e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 65: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 3.4053e-04 - mean_absolute_error: 0.0196 - val_loss: 5.8052e-04 - val_mean_absolute_error: 0.0257\n",
      "Epoch 66/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 3.3643e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 66: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.3648e-04 - mean_absolute_error: 0.0194 - val_loss: 6.5649e-04 - val_mean_absolute_error: 0.0272\n",
      "Epoch 67/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.5481e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 67: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 3.5459e-04 - mean_absolute_error: 0.0198 - val_loss: 5.3157e-04 - val_mean_absolute_error: 0.0244\n",
      "Epoch 68/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.1172e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 68: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.1169e-04 - mean_absolute_error: 0.0187 - val_loss: 5.8725e-04 - val_mean_absolute_error: 0.0262\n",
      "Epoch 69/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.1757e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 69: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.1746e-04 - mean_absolute_error: 0.0189 - val_loss: 5.6269e-04 - val_mean_absolute_error: 0.0251\n",
      "Epoch 70/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.1299e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 70: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.1285e-04 - mean_absolute_error: 0.0187 - val_loss: 5.5167e-04 - val_mean_absolute_error: 0.0259\n",
      "Epoch 71/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.0686e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 71: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 3.0686e-04 - mean_absolute_error: 0.0184 - val_loss: 5.9206e-04 - val_mean_absolute_error: 0.0262\n",
      "Epoch 72/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.0267e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 72: val_loss did not improve from 0.00049\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 3.0280e-04 - mean_absolute_error: 0.0184 - val_loss: 5.0293e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 73/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.1997e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 73: val_loss improved from 0.00049 to 0.00046, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.1997e-04 - mean_absolute_error: 0.0188 - val_loss: 4.6352e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 74/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.1998e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 74: val_loss did not improve from 0.00046\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 3.2008e-04 - mean_absolute_error: 0.0186 - val_loss: 5.8957e-04 - val_mean_absolute_error: 0.0255\n",
      "Epoch 75/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.7807e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 75: val_loss did not improve from 0.00046\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 2.7783e-04 - mean_absolute_error: 0.0176 - val_loss: 5.1967e-04 - val_mean_absolute_error: 0.0245\n",
      "Epoch 76/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.7573e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 76: val_loss did not improve from 0.00046\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.7569e-04 - mean_absolute_error: 0.0175 - val_loss: 4.6823e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 77/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.7249e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 77: val_loss did not improve from 0.00046\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 2.7238e-04 - mean_absolute_error: 0.0174 - val_loss: 5.1546e-04 - val_mean_absolute_error: 0.0244\n",
      "Epoch 78/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.6627e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 78: val_loss did not improve from 0.00046\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.6648e-04 - mean_absolute_error: 0.0172 - val_loss: 4.9936e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 79/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.6605e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 79: val_loss improved from 0.00046 to 0.00043, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.6610e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3171e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 80/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.7463e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 80: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 20s 27ms/step - loss: 2.7463e-04 - mean_absolute_error: 0.0175 - val_loss: 5.7338e-04 - val_mean_absolute_error: 0.0257\n",
      "Epoch 81/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.5897e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 81: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.5906e-04 - mean_absolute_error: 0.0169 - val_loss: 4.8836e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 82/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.5354e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 82: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.5354e-04 - mean_absolute_error: 0.0168 - val_loss: 4.4569e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727/728 [============================>.] - ETA: 0s - loss: 2.6348e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 83: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.6353e-04 - mean_absolute_error: 0.0170 - val_loss: 5.0207e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 84/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.4452e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 84: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.4523e-04 - mean_absolute_error: 0.0164 - val_loss: 5.0950e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 85/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.5049e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 85: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.5049e-04 - mean_absolute_error: 0.0167 - val_loss: 4.4183e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 86/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.5004e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 86: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 2.5005e-04 - mean_absolute_error: 0.0166 - val_loss: 4.4885e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 87/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.2756e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 87: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.2756e-04 - mean_absolute_error: 0.0159 - val_loss: 4.3580e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 88/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.3664e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 88: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 22s 31ms/step - loss: 2.3664e-04 - mean_absolute_error: 0.0162 - val_loss: 5.6671e-04 - val_mean_absolute_error: 0.0259\n",
      "Epoch 89/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.3568e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 89: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.3565e-04 - mean_absolute_error: 0.0162 - val_loss: 4.8609e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 90/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.2894e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 90: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 30ms/step - loss: 2.2888e-04 - mean_absolute_error: 0.0159 - val_loss: 4.8222e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 91/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.1705e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 91: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.1708e-04 - mean_absolute_error: 0.0155 - val_loss: 4.7739e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 92/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.1731e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 92: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 22s 31ms/step - loss: 2.1736e-04 - mean_absolute_error: 0.0156 - val_loss: 4.4065e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 93/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.3207e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 93: val_loss did not improve from 0.00043\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.3208e-04 - mean_absolute_error: 0.0160 - val_loss: 4.7804e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 94/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.2752e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 94: val_loss improved from 0.00043 to 0.00040, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 22s 30ms/step - loss: 2.2750e-04 - mean_absolute_error: 0.0158 - val_loss: 3.9887e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 95/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.0756e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 95: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 2.0789e-04 - mean_absolute_error: 0.0152 - val_loss: 4.5724e-04 - val_mean_absolute_error: 0.0226\n",
      "Epoch 96/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.2404e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 96: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 2.2404e-04 - mean_absolute_error: 0.0156 - val_loss: 4.4341e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 97/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 2.0600e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 97: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 2.0600e-04 - mean_absolute_error: 0.0151 - val_loss: 5.0285e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 98/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.1615e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 98: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.1622e-04 - mean_absolute_error: 0.0153 - val_loss: 5.9455e-04 - val_mean_absolute_error: 0.0270\n",
      "Epoch 99/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 2.1413e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 99: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.1403e-04 - mean_absolute_error: 0.0153 - val_loss: 4.1336e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 100/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.0139e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 100: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 2.0143e-04 - mean_absolute_error: 0.0148 - val_loss: 5.2869e-04 - val_mean_absolute_error: 0.0251\n",
      "Epoch 101/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 1.9515e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 101: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 1.9515e-04 - mean_absolute_error: 0.0147 - val_loss: 4.4691e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 102/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 1.9502e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 102: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 1.9505e-04 - mean_absolute_error: 0.0147 - val_loss: 4.3870e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 103/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 1.9893e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 103: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 1.9884e-04 - mean_absolute_error: 0.0149 - val_loss: 4.4277e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 104/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 1.9453e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 104: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 1.9447e-04 - mean_absolute_error: 0.0146 - val_loss: 4.7737e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 105/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 2.1489e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 105: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 2.1486e-04 - mean_absolute_error: 0.0152 - val_loss: 4.7374e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 106/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 1.8436e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 106: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 1.8432e-04 - mean_absolute_error: 0.0143 - val_loss: 4.1365e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 107/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 1.9414e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 107: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 1.9415e-04 - mean_absolute_error: 0.0146 - val_loss: 4.5809e-04 - val_mean_absolute_error: 0.0222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "727/728 [============================>.] - ETA: 0s - loss: 1.8686e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 108: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 28ms/step - loss: 1.8686e-04 - mean_absolute_error: 0.0142 - val_loss: 4.3733e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 109/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 1.8409e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 109: val_loss improved from 0.00040 to 0.00040, saving model to results/ipostest4a-adjclose-sh-1-sc-1-sbd-0-seq-50-step-15-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-huber_loss-adam-LSTM-layers-2-units-256.h5\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 1.8405e-04 - mean_absolute_error: 0.0143 - val_loss: 3.9696e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 110/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 1.7954e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 110: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 1.7954e-04 - mean_absolute_error: 0.0141 - val_loss: 4.9753e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 111/200\n",
      "728/728 [==============================] - ETA: 0s - loss: 1.8712e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 111: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 21s 29ms/step - loss: 1.8712e-04 - mean_absolute_error: 0.0143 - val_loss: 4.8085e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 112/200\n",
      "726/728 [============================>.] - ETA: 0s - loss: 1.7690e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 112: val_loss did not improve from 0.00040\n",
      "728/728 [==============================] - 20s 28ms/step - loss: 1.7691e-04 - mean_absolute_error: 0.0139 - val_loss: 4.3348e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 113/200\n",
      "524/728 [====================>.........] - ETA: 5s - loss: 1.7890e-04 - mean_absolute_error: 0.0140"
     ]
    }
   ],
   "source": [
    "findata.EPOCHS=200\n",
    "pipeline.IS_VERBOSE=False\n",
    "tickers = ['ABNB', 'ACLS' ,'AI', 'AMBA', 'APP',\n",
    "           'BILL', 'BMBL', 'CELH', 'CFLT', 'CHGG', 'CRCT', 'CRWD',\n",
    "           'DASH', 'DBX', 'DDOG', 'DOCN', 'DOCS', 'DOCU', 'DXCM',\n",
    "           'ESTC', 'ETSY', 'EXPE', 'FOUR', 'GFS', 'GLBE', 'GOGO',\n",
    "           'INDI', 'INMD', 'INTA', 'IOT','JKS', 'LUMN', 'LYFT',\n",
    "           'MAXR', 'MBLY', 'MDB',  'MNDY', 'MNST', 'MPWR',  'MXL',\n",
    "           'MTCH',  'NVCR','OKTA', 'OLED', 'OSTK',\n",
    "           'PANW', 'PAYO', 'PD', 'PLUG', 'PI', 'PINS', 'PTON', 'PUBM', 'RBLX',\n",
    "           'SNAP', 'SNOW', 'SPLK', 'SPOT','STEM', 'STNE','SWAV',\n",
    "           'TEAM', 'TDOC', 'TOST', 'TTD', 'TWLO',\n",
    "           'U','UI', 'UBER', 'UPWK', 'WOLF', 'VEEV', 'Z', 'ZM', 'ZS']\n",
    "# # RateReturnOnlyExperimental\n",
    "# pipeline.PenaltyTrading.threshold=0.2\n",
    "\n",
    "lossfn = \"huber_loss\"\n",
    "mod = pipeline.RateReturnOnly(\n",
    "    pipeline.FeatureSeq([pipeline.AddDayMonth(), pipeline.AddVWap(), pipeline.AddMA(200), pipeline.Adj()]))\n",
    "df = pipeline.runModelCombinedVola(tickers, 'ipostest4a', mod, True, loss=lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>pred_high</th>\n",
       "      <th>true_high</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>true_adjclose</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-03</th>\n",
       "      <td>132.689362</td>\n",
       "      <td>138.613083</td>\n",
       "      <td>135.650177</td>\n",
       "      <td>132.409363</td>\n",
       "      <td>135.039047</td>\n",
       "      <td>2.629684</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-05</th>\n",
       "      <td>132.294022</td>\n",
       "      <td>138.489929</td>\n",
       "      <td>136.486755</td>\n",
       "      <td>132.024017</td>\n",
       "      <td>136.146759</td>\n",
       "      <td>4.122742</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>134.026443</td>\n",
       "      <td>138.954788</td>\n",
       "      <td>136.486755</td>\n",
       "      <td>133.796432</td>\n",
       "      <td>131.869904</td>\n",
       "      <td>-1.926529</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-19</th>\n",
       "      <td>135.650177</td>\n",
       "      <td>139.937119</td>\n",
       "      <td>136.486755</td>\n",
       "      <td>131.600174</td>\n",
       "      <td>132.650162</td>\n",
       "      <td>1.049988</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22</th>\n",
       "      <td>134.103699</td>\n",
       "      <td>140.345413</td>\n",
       "      <td>136.486755</td>\n",
       "      <td>133.343689</td>\n",
       "      <td>130.800720</td>\n",
       "      <td>-2.542969</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>136.486755</td>\n",
       "      <td>144.330475</td>\n",
       "      <td>136.328186</td>\n",
       "      <td>136.146759</td>\n",
       "      <td>131.564865</td>\n",
       "      <td>-4.581894</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>134.320160</td>\n",
       "      <td>137.379501</td>\n",
       "      <td>134.835693</td>\n",
       "      <td>132.650162</td>\n",
       "      <td>133.236832</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-13</th>\n",
       "      <td>133.707474</td>\n",
       "      <td>138.547577</td>\n",
       "      <td>134.835693</td>\n",
       "      <td>133.507477</td>\n",
       "      <td>133.014557</td>\n",
       "      <td>-0.492920</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>130.097015</td>\n",
       "      <td>138.856415</td>\n",
       "      <td>135.683411</td>\n",
       "      <td>129.197006</td>\n",
       "      <td>134.435287</td>\n",
       "      <td>5.238281</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-16</th>\n",
       "      <td>131.894867</td>\n",
       "      <td>139.074966</td>\n",
       "      <td>135.683411</td>\n",
       "      <td>131.564865</td>\n",
       "      <td>132.908249</td>\n",
       "      <td>1.343384</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-21</th>\n",
       "      <td>134.835693</td>\n",
       "      <td>142.001862</td>\n",
       "      <td>135.683411</td>\n",
       "      <td>134.135681</td>\n",
       "      <td>132.908249</td>\n",
       "      <td>-1.227432</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>135.138245</td>\n",
       "      <td>141.307510</td>\n",
       "      <td>137.639175</td>\n",
       "      <td>132.908249</td>\n",
       "      <td>134.367645</td>\n",
       "      <td>1.459396</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-10</th>\n",
       "      <td>132.326523</td>\n",
       "      <td>140.507034</td>\n",
       "      <td>137.639175</td>\n",
       "      <td>131.516525</td>\n",
       "      <td>132.473343</td>\n",
       "      <td>0.956818</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13</th>\n",
       "      <td>133.454956</td>\n",
       "      <td>138.971863</td>\n",
       "      <td>137.639175</td>\n",
       "      <td>132.714966</td>\n",
       "      <td>133.488113</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>135.653336</td>\n",
       "      <td>144.133133</td>\n",
       "      <td>137.042435</td>\n",
       "      <td>132.473343</td>\n",
       "      <td>131.796814</td>\n",
       "      <td>-0.676529</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-17</th>\n",
       "      <td>136.735474</td>\n",
       "      <td>143.722473</td>\n",
       "      <td>140.834595</td>\n",
       "      <td>135.005478</td>\n",
       "      <td>139.422211</td>\n",
       "      <td>4.416733</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-22</th>\n",
       "      <td>135.436813</td>\n",
       "      <td>143.338608</td>\n",
       "      <td>142.637985</td>\n",
       "      <td>131.796814</td>\n",
       "      <td>142.137985</td>\n",
       "      <td>10.341171</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-23</th>\n",
       "      <td>132.847656</td>\n",
       "      <td>142.904343</td>\n",
       "      <td>142.637985</td>\n",
       "      <td>132.637665</td>\n",
       "      <td>142.369934</td>\n",
       "      <td>9.732269</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>135.727097</td>\n",
       "      <td>142.967697</td>\n",
       "      <td>143.444794</td>\n",
       "      <td>135.247101</td>\n",
       "      <td>143.094788</td>\n",
       "      <td>7.720596</td>\n",
       "      <td>-0.12709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>140.834595</td>\n",
       "      <td>144.850555</td>\n",
       "      <td>145.508362</td>\n",
       "      <td>139.354599</td>\n",
       "      <td>145.298355</td>\n",
       "      <td>5.495956</td>\n",
       "      <td>-0.44780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high   pred_high   true_high    adjclose  true_adjclose  \\\n",
       "2019-07-03  132.689362  138.613083  135.650177  132.409363     135.039047   \n",
       "2019-07-05  132.294022  138.489929  136.486755  132.024017     136.146759   \n",
       "2019-07-12  134.026443  138.954788  136.486755  133.796432     131.869904   \n",
       "2019-07-19  135.650177  139.937119  136.486755  131.600174     132.650162   \n",
       "2019-07-22  134.103699  140.345413  136.486755  133.343689     130.800720   \n",
       "2019-07-26  136.486755  144.330475  136.328186  136.146759     131.564865   \n",
       "2019-08-09  134.320160  137.379501  134.835693  132.650162     133.236832   \n",
       "2019-08-13  133.707474  138.547577  134.835693  133.507477     133.014557   \n",
       "2019-08-15  130.097015  138.856415  135.683411  129.197006     134.435287   \n",
       "2019-08-16  131.894867  139.074966  135.683411  131.564865     132.908249   \n",
       "2019-08-21  134.835693  142.001862  135.683411  134.135681     132.908249   \n",
       "2019-09-09  135.138245  141.307510  137.639175  132.908249     134.367645   \n",
       "2019-09-10  132.326523  140.507034  137.639175  131.516525     132.473343   \n",
       "2019-09-13  133.454956  138.971863  137.639175  132.714966     133.488113   \n",
       "2019-10-01  135.653336  144.133133  137.042435  132.473343     131.796814   \n",
       "2019-10-17  136.735474  143.722473  140.834595  135.005478     139.422211   \n",
       "2019-10-22  135.436813  143.338608  142.637985  131.796814     142.137985   \n",
       "2019-10-23  132.847656  142.904343  142.637985  132.637665     142.369934   \n",
       "2019-10-24  135.727097  142.967697  143.444794  135.247101     143.094788   \n",
       "2019-10-28  140.834595  144.850555  145.508362  139.354599     145.298355   \n",
       "\n",
       "            buy_profit  sell_profit  \n",
       "2019-07-03    2.629684      0.00000  \n",
       "2019-07-05    4.122742      0.00000  \n",
       "2019-07-12   -1.926529      0.00000  \n",
       "2019-07-19    1.049988      0.00000  \n",
       "2019-07-22   -2.542969      0.00000  \n",
       "2019-07-26   -4.581894      0.00000  \n",
       "2019-08-09    0.586670      0.00000  \n",
       "2019-08-13   -0.492920      0.00000  \n",
       "2019-08-15    5.238281      0.00000  \n",
       "2019-08-16    1.343384      0.00000  \n",
       "2019-08-21   -1.227432      0.00000  \n",
       "2019-09-09    1.459396      0.00000  \n",
       "2019-09-10    0.956818      0.00000  \n",
       "2019-09-13    0.773148      0.00000  \n",
       "2019-10-01   -0.676529      0.00000  \n",
       "2019-10-17    4.416733      0.00000  \n",
       "2019-10-22   10.341171      0.00000  \n",
       "2019-10-23    9.732269      0.00000  \n",
       "2019-10-24    7.720596     -0.12709  \n",
       "2019-10-28    5.495956     -0.44780  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#testdf = res['MSFT'].data\n",
    "adddf = res['MSFT'].final_df\n",
    "# #pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "# df = df.drop(columns=['ma-adjclose-200', 'vwap', 'month', 'mday', 'open', 'volume', 'buy_profit', 'sell_profit'])\n",
    "# for col in list(df):\n",
    "#     if df[col].dtype == np.float64:\n",
    "#         df[col] = df[col].round(2)\n",
    "\n",
    "# print (\"df = pd.DataFrame( %s )\" % (str(df.reset_index(drop=True).to_dict())))\n",
    "#testdf['test_df']\n",
    "adddf.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Buy</th>\n",
       "      <th>Sell</th>\n",
       "      <th>Total</th>\n",
       "      <th>Last</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Gain</th>\n",
       "      <th>Alloc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Col</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>13.370372</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>66.49</td>\n",
       "      <td>364.80</td>\n",
       "      <td>431.29</td>\n",
       "      <td>22.75</td>\n",
       "      <td>48.689999</td>\n",
       "      <td>1.14</td>\n",
       "      <td>6.892231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABNB</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>87.175750</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>603.22</td>\n",
       "      <td>627.28</td>\n",
       "      <td>1230.50</td>\n",
       "      <td>114.25</td>\n",
       "      <td>109.879997</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>84.672864</td>\n",
       "      <td>0.856383</td>\n",
       "      <td>760.69</td>\n",
       "      <td>718.45</td>\n",
       "      <td>1479.14</td>\n",
       "      <td>99.92</td>\n",
       "      <td>105.449997</td>\n",
       "      <td>0.06</td>\n",
       "      <td>6.170213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>121.268275</td>\n",
       "      <td>0.824468</td>\n",
       "      <td>1190.31</td>\n",
       "      <td>664.21</td>\n",
       "      <td>1854.52</td>\n",
       "      <td>282.83</td>\n",
       "      <td>270.119995</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.856383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>14.525238</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.93</td>\n",
       "      <td>101.93</td>\n",
       "      <td>22.75</td>\n",
       "      <td>36.220001</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMBA</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>38.026026</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>742.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>742.49</td>\n",
       "      <td>71.22</td>\n",
       "      <td>61.779999</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APP</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>10.598792</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>130.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>130.65</td>\n",
       "      <td>15.96</td>\n",
       "      <td>12.770000</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>87.579336</td>\n",
       "      <td>0.324468</td>\n",
       "      <td>0.00</td>\n",
       "      <td>352.09</td>\n",
       "      <td>352.09</td>\n",
       "      <td>99.92</td>\n",
       "      <td>104.879997</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>118.024078</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>508.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508.32</td>\n",
       "      <td>282.83</td>\n",
       "      <td>263.869995</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>11.689328</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>84.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>84.82</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.020000</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACLS</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>14.370740</td>\n",
       "      <td>0.372340</td>\n",
       "      <td>355.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>355.76</td>\n",
       "      <td>125.84</td>\n",
       "      <td>112.809998</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABNB</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>83.949761</td>\n",
       "      <td>0.361905</td>\n",
       "      <td>450.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>450.75</td>\n",
       "      <td>114.25</td>\n",
       "      <td>110.220001</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMBA</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>43.594979</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.00</td>\n",
       "      <td>352.15</td>\n",
       "      <td>352.15</td>\n",
       "      <td>71.22</td>\n",
       "      <td>74.959999</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>125.097952</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.00</td>\n",
       "      <td>331.07</td>\n",
       "      <td>331.07</td>\n",
       "      <td>282.83</td>\n",
       "      <td>293.390015</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APP</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>12.658953</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.29</td>\n",
       "      <td>77.29</td>\n",
       "      <td>15.96</td>\n",
       "      <td>17.389999</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ACLS</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>16.087067</td>\n",
       "      <td>0.734043</td>\n",
       "      <td>468.51</td>\n",
       "      <td>193.51</td>\n",
       "      <td>662.02</td>\n",
       "      <td>125.84</td>\n",
       "      <td>122.019997</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>18.104605</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.69</td>\n",
       "      <td>31.69</td>\n",
       "      <td>125.84</td>\n",
       "      <td>126.730003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABNB</th>\n",
       "      <th>high</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>89.940924</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.00</td>\n",
       "      <td>335.80</td>\n",
       "      <td>335.80</td>\n",
       "      <td>114.25</td>\n",
       "      <td>115.309998</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APP</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>12.133722</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>145.71</td>\n",
       "      <td>360.41</td>\n",
       "      <td>506.12</td>\n",
       "      <td>15.96</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMBA</th>\n",
       "      <th>adjclose</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>41.898789</td>\n",
       "      <td>0.734043</td>\n",
       "      <td>718.68</td>\n",
       "      <td>882.46</td>\n",
       "      <td>1601.14</td>\n",
       "      <td>71.22</td>\n",
       "      <td>71.389999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <th>low</th>\n",
       "      <td>RROnly</td>\n",
       "      <td>83.004285</td>\n",
       "      <td>0.335106</td>\n",
       "      <td>586.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>586.04</td>\n",
       "      <td>99.92</td>\n",
       "      <td>94.870003</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name       Error  Accuracy      Buy    Sell    Total  \\\n",
       "Ticker Col                                                                \n",
       "AI     adjclose  RROnly   13.370372  0.714286    66.49  364.80   431.29   \n",
       "ABNB   adjclose  RROnly   87.175750  0.904762   603.22  627.28  1230.50   \n",
       "AMZN   adjclose  RROnly   84.672864  0.856383   760.69  718.45  1479.14   \n",
       "MSFT   adjclose  RROnly  121.268275  0.824468  1190.31  664.21  1854.52   \n",
       "AI     high      RROnly   14.525238  0.266667     0.00  101.93   101.93   \n",
       "AMBA   low       RROnly   38.026026  0.500000   742.49    0.00   742.49   \n",
       "APP    low       RROnly   10.598792  0.375000   130.65    0.00   130.65   \n",
       "AMZN   high      RROnly   87.579336  0.324468     0.00  352.09   352.09   \n",
       "MSFT   low       RROnly  118.024078  0.234043   508.32    0.00   508.32   \n",
       "AI     low       RROnly   11.689328  0.304762    84.82    0.00    84.82   \n",
       "ACLS   low       RROnly   14.370740  0.372340   355.76    0.00   355.76   \n",
       "ABNB   low       RROnly   83.949761  0.361905   450.75    0.00   450.75   \n",
       "AMBA   high      RROnly   43.594979  0.244681     0.00  352.15   352.15   \n",
       "MSFT   high      RROnly  125.097952  0.319149     0.00  331.07   331.07   \n",
       "APP    high      RROnly   12.658953  0.136364     0.00   77.29    77.29   \n",
       "ACLS   adjclose  RROnly   16.087067  0.734043   468.51  193.51   662.02   \n",
       "       high      RROnly   18.104605  0.191489     0.00   31.69    31.69   \n",
       "ABNB   high      RROnly   89.940924  0.304762     0.00  335.80   335.80   \n",
       "APP    adjclose  RROnly   12.133722  0.795455   145.71  360.41   506.12   \n",
       "AMBA   adjclose  RROnly   41.898789  0.734043   718.68  882.46  1601.14   \n",
       "AMZN   low       RROnly   83.004285  0.335106   586.04    0.00   586.04   \n",
       "\n",
       "                   Last   Predicted  Gain     Alloc  \n",
       "Ticker Col                                           \n",
       "AI     adjclose   22.75   48.689999  1.14  6.892231  \n",
       "ABNB   adjclose  114.25  109.879997 -0.04  6.666667  \n",
       "AMZN   adjclose   99.92  105.449997  0.06  6.170213  \n",
       "MSFT   adjclose  282.83  270.119995 -0.04  3.856383  \n",
       "AI     high       22.75   36.220001  0.59  1.423729  \n",
       "AMBA   low        71.22   61.779999 -0.13  1.153846  \n",
       "APP    low        15.96   12.770000 -0.20  0.625000  \n",
       "AMZN   high       99.92  104.879997  0.05  0.000000  \n",
       "MSFT   low       282.83  263.869995 -0.07  0.000000  \n",
       "AI     low        22.75   22.020000 -0.03  0.000000  \n",
       "ACLS   low       125.84  112.809998 -0.10  0.000000  \n",
       "ABNB   low       114.25  110.220001 -0.04  0.000000  \n",
       "AMBA   high       71.22   74.959999  0.05  0.000000  \n",
       "MSFT   high      282.83  293.390015  0.04  0.000000  \n",
       "APP    high       15.96   17.389999  0.09  0.000000  \n",
       "ACLS   adjclose  125.84  122.019997 -0.03  0.000000  \n",
       "       high      125.84  126.730003  0.01  0.000000  \n",
       "ABNB   high      114.25  115.309998  0.01  0.000000  \n",
       "APP    adjclose   15.96   15.730000 -0.01  0.000000  \n",
       "AMBA   adjclose   71.22   71.389999  0.00  0.000000  \n",
       "AMZN   low        99.92   94.870003 -0.05  0.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addAlloc(df, 0.10, 1)\n",
    "df.sort_values('Alloc', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>120.639999</td>\n",
       "      <td>120.980003</td>\n",
       "      <td>120.370003</td>\n",
       "      <td>120.949997</td>\n",
       "      <td>116.076286</td>\n",
       "      <td>19745100</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>120.940002</td>\n",
       "      <td>121.580002</td>\n",
       "      <td>120.570000</td>\n",
       "      <td>121.050003</td>\n",
       "      <td>116.172279</td>\n",
       "      <td>15792600</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>121.639999</td>\n",
       "      <td>121.650002</td>\n",
       "      <td>120.099998</td>\n",
       "      <td>120.769997</td>\n",
       "      <td>115.903542</td>\n",
       "      <td>14071800</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>121.239998</td>\n",
       "      <td>121.849998</td>\n",
       "      <td>120.540001</td>\n",
       "      <td>121.769997</td>\n",
       "      <td>116.863243</td>\n",
       "      <td>19300900</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>122.190002</td>\n",
       "      <td>123.519997</td>\n",
       "      <td>121.300003</td>\n",
       "      <td>123.370003</td>\n",
       "      <td>118.398781</td>\n",
       "      <td>27991000</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27</th>\n",
       "      <td>280.500000</td>\n",
       "      <td>281.459991</td>\n",
       "      <td>275.519989</td>\n",
       "      <td>276.380005</td>\n",
       "      <td>276.380005</td>\n",
       "      <td>26840200</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28</th>\n",
       "      <td>275.790009</td>\n",
       "      <td>276.140015</td>\n",
       "      <td>272.049988</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>21878600</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>278.959991</td>\n",
       "      <td>281.140015</td>\n",
       "      <td>278.410004</td>\n",
       "      <td>280.510010</td>\n",
       "      <td>280.510010</td>\n",
       "      <td>25087000</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>284.230011</td>\n",
       "      <td>284.459991</td>\n",
       "      <td>281.480011</td>\n",
       "      <td>284.049988</td>\n",
       "      <td>284.049988</td>\n",
       "      <td>25053400</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>283.730011</td>\n",
       "      <td>289.269989</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>288.299988</td>\n",
       "      <td>288.299988</td>\n",
       "      <td>32740300</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "2019-04-12  120.639999  120.980003  120.370003  120.949997  116.076286   \n",
       "2019-04-15  120.940002  121.580002  120.570000  121.050003  116.172279   \n",
       "2019-04-16  121.639999  121.650002  120.099998  120.769997  115.903542   \n",
       "2019-04-17  121.239998  121.849998  120.540001  121.769997  116.863243   \n",
       "2019-04-18  122.190002  123.519997  121.300003  123.370003  118.398781   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2023-03-27  280.500000  281.459991  275.519989  276.380005  276.380005   \n",
       "2023-03-28  275.790009  276.140015  272.049988  275.230011  275.230011   \n",
       "2023-03-29  278.959991  281.140015  278.410004  280.510010  280.510010   \n",
       "2023-03-30  284.230011  284.459991  281.480011  284.049988  284.049988   \n",
       "2023-03-31  283.730011  289.269989  283.000000  288.299988  288.299988   \n",
       "\n",
       "              volume ticker  \n",
       "2019-04-12  19745100   MSFT  \n",
       "2019-04-15  15792600   MSFT  \n",
       "2019-04-16  14071800   MSFT  \n",
       "2019-04-17  19300900   MSFT  \n",
       "2019-04-18  27991000   MSFT  \n",
       "...              ...    ...  \n",
       "2023-03-27  26840200   MSFT  \n",
       "2023-03-28  21878600   MSFT  \n",
       "2023-03-29  25087000   MSFT  \n",
       "2023-03-30  25053400   MSFT  \n",
       "2023-03-31  32740300   MSFT  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pipeline.fetch_data('MSFT')\n",
    "data = data.tail(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjhigh</th>\n",
       "      <th>adjlow</th>\n",
       "      <th>future</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>234.860001</td>\n",
       "      <td>240.740005</td>\n",
       "      <td>234.509995</td>\n",
       "      <td>240.220001</td>\n",
       "      <td>239.619827</td>\n",
       "      <td>35389800</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>240.138532</td>\n",
       "      <td>233.924087</td>\n",
       "      <td>262.442657</td>\n",
       "      <td>230.323103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>241.100006</td>\n",
       "      <td>245.169998</td>\n",
       "      <td>239.649994</td>\n",
       "      <td>242.580002</td>\n",
       "      <td>241.973923</td>\n",
       "      <td>31934000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>244.557448</td>\n",
       "      <td>239.051235</td>\n",
       "      <td>270.642120</td>\n",
       "      <td>230.323103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-24</th>\n",
       "      <td>242.500000</td>\n",
       "      <td>243.949997</td>\n",
       "      <td>240.440002</td>\n",
       "      <td>242.039993</td>\n",
       "      <td>241.435272</td>\n",
       "      <td>40234400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>243.340504</td>\n",
       "      <td>239.839279</td>\n",
       "      <td>271.490021</td>\n",
       "      <td>230.323103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-25</th>\n",
       "      <td>234.479996</td>\n",
       "      <td>243.300003</td>\n",
       "      <td>230.899994</td>\n",
       "      <td>240.610001</td>\n",
       "      <td>240.008850</td>\n",
       "      <td>66526600</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>242.692132</td>\n",
       "      <td>230.323103</td>\n",
       "      <td>269.320007</td>\n",
       "      <td>241.395377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-26</th>\n",
       "      <td>243.649994</td>\n",
       "      <td>248.309998</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>247.380386</td>\n",
       "      <td>33454500</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>247.689609</td>\n",
       "      <td>241.395377</td>\n",
       "      <td>262.149994</td>\n",
       "      <td>241.594872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-27</th>\n",
       "      <td>248.990005</td>\n",
       "      <td>249.830002</td>\n",
       "      <td>246.830002</td>\n",
       "      <td>248.160004</td>\n",
       "      <td>247.539993</td>\n",
       "      <td>26498900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>249.205819</td>\n",
       "      <td>246.213314</td>\n",
       "      <td>258.059998</td>\n",
       "      <td>241.594872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-30</th>\n",
       "      <td>244.509995</td>\n",
       "      <td>245.600006</td>\n",
       "      <td>242.199997</td>\n",
       "      <td>242.710007</td>\n",
       "      <td>242.103607</td>\n",
       "      <td>25867400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>244.986386</td>\n",
       "      <td>241.594872</td>\n",
       "      <td>252.669998</td>\n",
       "      <td>242.342999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>243.449997</td>\n",
       "      <td>247.949997</td>\n",
       "      <td>242.949997</td>\n",
       "      <td>247.809998</td>\n",
       "      <td>247.190857</td>\n",
       "      <td>26541100</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>247.330507</td>\n",
       "      <td>242.342999</td>\n",
       "      <td>251.509995</td>\n",
       "      <td>244.856705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-01</th>\n",
       "      <td>248.000000</td>\n",
       "      <td>255.179993</td>\n",
       "      <td>245.470001</td>\n",
       "      <td>252.750000</td>\n",
       "      <td>252.118515</td>\n",
       "      <td>31259900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>254.542436</td>\n",
       "      <td>244.856705</td>\n",
       "      <td>254.770004</td>\n",
       "      <td>250.339996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-02</th>\n",
       "      <td>258.820007</td>\n",
       "      <td>264.690002</td>\n",
       "      <td>257.250000</td>\n",
       "      <td>264.600006</td>\n",
       "      <td>263.938904</td>\n",
       "      <td>39940400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>264.028675</td>\n",
       "      <td>256.607262</td>\n",
       "      <td>249.220001</td>\n",
       "      <td>248.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03</th>\n",
       "      <td>259.540009</td>\n",
       "      <td>264.200012</td>\n",
       "      <td>257.100006</td>\n",
       "      <td>258.350006</td>\n",
       "      <td>257.704529</td>\n",
       "      <td>29077300</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>263.539919</td>\n",
       "      <td>256.457652</td>\n",
       "      <td>250.160004</td>\n",
       "      <td>248.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-06</th>\n",
       "      <td>257.440002</td>\n",
       "      <td>258.299988</td>\n",
       "      <td>254.779999</td>\n",
       "      <td>256.769989</td>\n",
       "      <td>256.128448</td>\n",
       "      <td>22518000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>257.654625</td>\n",
       "      <td>254.143430</td>\n",
       "      <td>249.419998</td>\n",
       "      <td>248.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-07</th>\n",
       "      <td>260.529999</td>\n",
       "      <td>268.769989</td>\n",
       "      <td>260.079987</td>\n",
       "      <td>267.559998</td>\n",
       "      <td>266.891510</td>\n",
       "      <td>50841400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>268.098478</td>\n",
       "      <td>259.430188</td>\n",
       "      <td>246.270004</td>\n",
       "      <td>245.789993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-08</th>\n",
       "      <td>273.200012</td>\n",
       "      <td>276.760010</td>\n",
       "      <td>266.209991</td>\n",
       "      <td>266.730011</td>\n",
       "      <td>266.063599</td>\n",
       "      <td>54686000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>276.068538</td>\n",
       "      <td>265.544878</td>\n",
       "      <td>251.110001</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-09</th>\n",
       "      <td>273.799988</td>\n",
       "      <td>273.980011</td>\n",
       "      <td>262.799988</td>\n",
       "      <td>263.619995</td>\n",
       "      <td>262.961365</td>\n",
       "      <td>42375100</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>273.295497</td>\n",
       "      <td>262.143406</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-10</th>\n",
       "      <td>261.529999</td>\n",
       "      <td>264.089996</td>\n",
       "      <td>260.660004</td>\n",
       "      <td>263.100006</td>\n",
       "      <td>262.442657</td>\n",
       "      <td>25818500</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>263.430174</td>\n",
       "      <td>260.008751</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-13</th>\n",
       "      <td>267.640015</td>\n",
       "      <td>274.600006</td>\n",
       "      <td>267.149994</td>\n",
       "      <td>271.320007</td>\n",
       "      <td>270.642120</td>\n",
       "      <td>44630900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>273.913924</td>\n",
       "      <td>266.482526</td>\n",
       "      <td>254.149994</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-14</th>\n",
       "      <td>272.670013</td>\n",
       "      <td>274.970001</td>\n",
       "      <td>269.279999</td>\n",
       "      <td>272.170013</td>\n",
       "      <td>271.490021</td>\n",
       "      <td>37047900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>274.283013</td>\n",
       "      <td>268.607227</td>\n",
       "      <td>253.699997</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>268.320007</td>\n",
       "      <td>270.730011</td>\n",
       "      <td>266.179993</td>\n",
       "      <td>269.320007</td>\n",
       "      <td>269.320007</td>\n",
       "      <td>28922400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>270.730011</td>\n",
       "      <td>266.179993</td>\n",
       "      <td>252.320007</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-16</th>\n",
       "      <td>264.019989</td>\n",
       "      <td>266.739990</td>\n",
       "      <td>261.899994</td>\n",
       "      <td>262.149994</td>\n",
       "      <td>262.149994</td>\n",
       "      <td>29603600</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>266.739990</td>\n",
       "      <td>261.899994</td>\n",
       "      <td>248.589996</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-17</th>\n",
       "      <td>259.390015</td>\n",
       "      <td>260.089996</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>258.059998</td>\n",
       "      <td>258.059998</td>\n",
       "      <td>30000100</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>260.089996</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>253.919998</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-21</th>\n",
       "      <td>254.479996</td>\n",
       "      <td>255.490005</td>\n",
       "      <td>251.589996</td>\n",
       "      <td>252.669998</td>\n",
       "      <td>252.669998</td>\n",
       "      <td>28397400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>255.490005</td>\n",
       "      <td>251.589996</td>\n",
       "      <td>260.790009</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22</th>\n",
       "      <td>254.089996</td>\n",
       "      <td>254.339996</td>\n",
       "      <td>250.339996</td>\n",
       "      <td>251.509995</td>\n",
       "      <td>251.509995</td>\n",
       "      <td>22491100</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>254.339996</td>\n",
       "      <td>250.339996</td>\n",
       "      <td>265.440002</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23</th>\n",
       "      <td>255.559998</td>\n",
       "      <td>256.839996</td>\n",
       "      <td>250.479996</td>\n",
       "      <td>254.770004</td>\n",
       "      <td>254.770004</td>\n",
       "      <td>29219100</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>256.839996</td>\n",
       "      <td>250.479996</td>\n",
       "      <td>276.200012</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24</th>\n",
       "      <td>249.960007</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>248.100006</td>\n",
       "      <td>249.220001</td>\n",
       "      <td>249.220001</td>\n",
       "      <td>24990900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>248.100006</td>\n",
       "      <td>279.429993</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-27</th>\n",
       "      <td>252.460007</td>\n",
       "      <td>252.820007</td>\n",
       "      <td>249.389999</td>\n",
       "      <td>250.160004</td>\n",
       "      <td>250.160004</td>\n",
       "      <td>21190000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>252.820007</td>\n",
       "      <td>249.389999</td>\n",
       "      <td>272.230011</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28</th>\n",
       "      <td>249.070007</td>\n",
       "      <td>251.490005</td>\n",
       "      <td>248.729996</td>\n",
       "      <td>249.419998</td>\n",
       "      <td>249.419998</td>\n",
       "      <td>22491000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>251.490005</td>\n",
       "      <td>248.729996</td>\n",
       "      <td>273.779999</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01</th>\n",
       "      <td>250.759995</td>\n",
       "      <td>250.929993</td>\n",
       "      <td>245.789993</td>\n",
       "      <td>246.270004</td>\n",
       "      <td>246.270004</td>\n",
       "      <td>27565300</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>250.929993</td>\n",
       "      <td>245.789993</td>\n",
       "      <td>272.290009</td>\n",
       "      <td>245.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02</th>\n",
       "      <td>246.550003</td>\n",
       "      <td>251.399994</td>\n",
       "      <td>245.610001</td>\n",
       "      <td>251.110001</td>\n",
       "      <td>251.110001</td>\n",
       "      <td>24808200</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>251.399994</td>\n",
       "      <td>245.610001</td>\n",
       "      <td>277.660004</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-03</th>\n",
       "      <td>252.190002</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>251.389999</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>30741300</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>251.389999</td>\n",
       "      <td>280.570007</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-06</th>\n",
       "      <td>256.429993</td>\n",
       "      <td>260.119995</td>\n",
       "      <td>255.979996</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>24109800</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>260.119995</td>\n",
       "      <td>255.979996</td>\n",
       "      <td>276.380005</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-07</th>\n",
       "      <td>256.299988</td>\n",
       "      <td>257.690002</td>\n",
       "      <td>253.389999</td>\n",
       "      <td>254.149994</td>\n",
       "      <td>254.149994</td>\n",
       "      <td>21473200</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>257.690002</td>\n",
       "      <td>253.389999</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-08</th>\n",
       "      <td>254.039993</td>\n",
       "      <td>254.539993</td>\n",
       "      <td>250.809998</td>\n",
       "      <td>253.699997</td>\n",
       "      <td>253.699997</td>\n",
       "      <td>17340200</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>254.539993</td>\n",
       "      <td>250.809998</td>\n",
       "      <td>280.510010</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-09</th>\n",
       "      <td>255.820007</td>\n",
       "      <td>259.559998</td>\n",
       "      <td>251.580002</td>\n",
       "      <td>252.320007</td>\n",
       "      <td>252.320007</td>\n",
       "      <td>26653400</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>259.559998</td>\n",
       "      <td>251.580002</td>\n",
       "      <td>284.049988</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-10</th>\n",
       "      <td>251.080002</td>\n",
       "      <td>252.789993</td>\n",
       "      <td>247.600006</td>\n",
       "      <td>248.589996</td>\n",
       "      <td>248.589996</td>\n",
       "      <td>28321800</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>252.789993</td>\n",
       "      <td>247.600006</td>\n",
       "      <td>288.299988</td>\n",
       "      <td>245.729996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "2023-01-20  234.860001  240.740005  234.509995  240.220001  239.619827   \n",
       "2023-01-23  241.100006  245.169998  239.649994  242.580002  241.973923   \n",
       "2023-01-24  242.500000  243.949997  240.440002  242.039993  241.435272   \n",
       "2023-01-25  234.479996  243.300003  230.899994  240.610001  240.008850   \n",
       "2023-01-26  243.649994  248.309998  242.000000  248.000000  247.380386   \n",
       "2023-01-27  248.990005  249.830002  246.830002  248.160004  247.539993   \n",
       "2023-01-30  244.509995  245.600006  242.199997  242.710007  242.103607   \n",
       "2023-01-31  243.449997  247.949997  242.949997  247.809998  247.190857   \n",
       "2023-02-01  248.000000  255.179993  245.470001  252.750000  252.118515   \n",
       "2023-02-02  258.820007  264.690002  257.250000  264.600006  263.938904   \n",
       "2023-02-03  259.540009  264.200012  257.100006  258.350006  257.704529   \n",
       "2023-02-06  257.440002  258.299988  254.779999  256.769989  256.128448   \n",
       "2023-02-07  260.529999  268.769989  260.079987  267.559998  266.891510   \n",
       "2023-02-08  273.200012  276.760010  266.209991  266.730011  266.063599   \n",
       "2023-02-09  273.799988  273.980011  262.799988  263.619995  262.961365   \n",
       "2023-02-10  261.529999  264.089996  260.660004  263.100006  262.442657   \n",
       "2023-02-13  267.640015  274.600006  267.149994  271.320007  270.642120   \n",
       "2023-02-14  272.670013  274.970001  269.279999  272.170013  271.490021   \n",
       "2023-02-15  268.320007  270.730011  266.179993  269.320007  269.320007   \n",
       "2023-02-16  264.019989  266.739990  261.899994  262.149994  262.149994   \n",
       "2023-02-17  259.390015  260.089996  256.000000  258.059998  258.059998   \n",
       "2023-02-21  254.479996  255.490005  251.589996  252.669998  252.669998   \n",
       "2023-02-22  254.089996  254.339996  250.339996  251.509995  251.509995   \n",
       "2023-02-23  255.559998  256.839996  250.479996  254.770004  254.770004   \n",
       "2023-02-24  249.960007  251.000000  248.100006  249.220001  249.220001   \n",
       "2023-02-27  252.460007  252.820007  249.389999  250.160004  250.160004   \n",
       "2023-02-28  249.070007  251.490005  248.729996  249.419998  249.419998   \n",
       "2023-03-01  250.759995  250.929993  245.789993  246.270004  246.270004   \n",
       "2023-03-02  246.550003  251.399994  245.610001  251.110001  251.110001   \n",
       "2023-03-03  252.190002  255.619995  251.389999  255.289993  255.289993   \n",
       "2023-03-06  256.429993  260.119995  255.979996  256.869995  256.869995   \n",
       "2023-03-07  256.299988  257.690002  253.389999  254.149994  254.149994   \n",
       "2023-03-08  254.039993  254.539993  250.809998  253.699997  253.699997   \n",
       "2023-03-09  255.820007  259.559998  251.580002  252.320007  252.320007   \n",
       "2023-03-10  251.080002  252.789993  247.600006  248.589996  248.589996   \n",
       "\n",
       "              volume ticker     adjhigh      adjlow      future         min  \n",
       "2023-01-20  35389800   MSFT  240.138532  233.924087  262.442657  230.323103  \n",
       "2023-01-23  31934000   MSFT  244.557448  239.051235  270.642120  230.323103  \n",
       "2023-01-24  40234400   MSFT  243.340504  239.839279  271.490021  230.323103  \n",
       "2023-01-25  66526600   MSFT  242.692132  230.323103  269.320007  241.395377  \n",
       "2023-01-26  33454500   MSFT  247.689609  241.395377  262.149994  241.594872  \n",
       "2023-01-27  26498900   MSFT  249.205819  246.213314  258.059998  241.594872  \n",
       "2023-01-30  25867400   MSFT  244.986386  241.594872  252.669998  242.342999  \n",
       "2023-01-31  26541100   MSFT  247.330507  242.342999  251.509995  244.856705  \n",
       "2023-02-01  31259900   MSFT  254.542436  244.856705  254.770004  250.339996  \n",
       "2023-02-02  39940400   MSFT  264.028675  256.607262  249.220001  248.100006  \n",
       "2023-02-03  29077300   MSFT  263.539919  256.457652  250.160004  248.100006  \n",
       "2023-02-06  22518000   MSFT  257.654625  254.143430  249.419998  248.100006  \n",
       "2023-02-07  50841400   MSFT  268.098478  259.430188  246.270004  245.789993  \n",
       "2023-02-08  54686000   MSFT  276.068538  265.544878  251.110001  245.610001  \n",
       "2023-02-09  42375100   MSFT  273.295497  262.143406  255.289993  245.610001  \n",
       "2023-02-10  25818500   MSFT  263.430174  260.008751  256.869995  245.610001  \n",
       "2023-02-13  44630900   MSFT  273.913924  266.482526  254.149994  245.610001  \n",
       "2023-02-14  37047900   MSFT  274.283013  268.607227  253.699997  245.610001  \n",
       "2023-02-15  28922400   MSFT  270.730011  266.179993  252.320007  245.610001  \n",
       "2023-02-16  29603600   MSFT  266.739990  261.899994  248.589996  245.610001  \n",
       "2023-02-17  30000100   MSFT  260.089996  256.000000  253.919998  245.610001  \n",
       "2023-02-21  28397400   MSFT  255.490005  251.589996  260.790009  245.610001  \n",
       "2023-02-22  22491100   MSFT  254.339996  250.339996  265.440002  245.610001  \n",
       "2023-02-23  29219100   MSFT  256.839996  250.479996  276.200012  245.610001  \n",
       "2023-02-24  24990900   MSFT  251.000000  248.100006  279.429993  245.610001  \n",
       "2023-02-27  21190000   MSFT  252.820007  249.389999  272.230011  245.610001  \n",
       "2023-02-28  22491000   MSFT  251.490005  248.729996  273.779999  245.610001  \n",
       "2023-03-01  27565300   MSFT  250.929993  245.789993  272.290009  245.610001  \n",
       "2023-03-02  24808200   MSFT  251.399994  245.610001  277.660004  245.729996  \n",
       "2023-03-03  30741300   MSFT  255.619995  251.389999  280.570007  245.729996  \n",
       "2023-03-06  24109800   MSFT  260.119995  255.979996  276.380005  245.729996  \n",
       "2023-03-07  21473200   MSFT  257.690002  253.389999  275.230011  245.729996  \n",
       "2023-03-08  17340200   MSFT  254.539993  250.809998  280.510010  245.729996  \n",
       "2023-03-09  26653400   MSFT  259.559998  251.580002  284.049988  245.729996  \n",
       "2023-03-10  28321800   MSFT  252.789993  247.600006  288.299988  245.729996  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['adjhigh'] = data.high/data.close*data.adjclose\n",
    "data['adjlow'] = data.low/data.close*data.adjclose\n",
    "data['future'] = data.adjclose.shift(-15)\n",
    "data['min']  = data.adjlow.rolling(15).min().shift(-15)\n",
    "data.tail(50).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Sw99bVXNIEym",
    "outputId": "08cc14eb-ffb6-4d33-9977-66faa88c2a60"
   },
   "outputs": [],
   "source": [
    "display(df.sort_values('Gain', ascending=False))\n",
    "display(df[[\"Buy\", \"Sell\", \"Total\"]].sum()/len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['ABNB'].pdata.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f5Ns68gHHpf"
   },
   "outputs": [],
   "source": [
    "findata.EPOCHS=10\n",
    "tickers1 = ['ARKK', 'ARKW', 'DAPP', 'DTEC', 'EEM', 'FPX',\n",
    "            'GBTC', 'GLD',\n",
    "            'ICLN', 'IJR', 'IPO', 'IPOS', 'IWM',\n",
    "            'JETS', 'KEMQ',\n",
    "            'MGK', 'MGV', 'MOAT', 'MTUM',\n",
    "            'QQQ', 'SLV','SMH', 'SMOG',\n",
    "            'SPY', 'VNQ', 'VT', 'VTI', 'WDIV',\n",
    "            'XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK',\n",
    "            'XLRE', 'XLU', 'XLV', 'XLY', 'XME' ]\n",
    "mod = pipeline.RateReturnOnly(pipeline.FeatureSeq([pipeline.AddDayMonth(), pipeline.AddVWap()]))\n",
    "df1 = pipeline.runModelCombined(tickers1, 'etf2b', mod, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X0wy3LeuIMg7",
    "outputId": "4a8f020c-b4fa-4036-bbf9-e6bf48af32a5"
   },
   "outputs": [],
   "source": [
    "df1.sort_values('Gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata.EPOCHS=200\n",
    "tickers3 = [\n",
    "           'AHT', 'AMSC', 'ASTR', 'ATOM',\n",
    "           'BKKT', 'BGFV', 'BGSF', 'CBNT', 'CLOV', \n",
    "           'DNMR', 'ERJ', 'EVGO',\n",
    "           'FSLY',  'FTCH', 'GOGO', 'HIVE', \n",
    "           'ILAL', 'INLX', 'JMIA', 'JOBY',  'KULR', 'MTTR',\n",
    "           'MYTE', 'NEPH', 'ONDS', 'MQ',\n",
    "           'PETS', 'PTON', 'SFIX', 'SFT', 'STNE', \n",
    "           'ULH', 'VRAR', 'WISH']\n",
    "lossfn = \"huber_loss\"\n",
    "# lossfn = \"mean_squared_error\"\n",
    "mod = pipeline.RateReturnOnly(\n",
    "    pipeline.FeatureSeq([pipeline.AddDayMonth(), pipeline.AddVWap(), pipeline.AddMA(200)]))\n",
    "df3, results = pipeline.runModelCombined(tickers3, 'vols', mod, False, loss=lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "seq = list()\n",
    "for ticker, result in results.items():\n",
    "    tempdf = result.final_df\n",
    "    tempdf['predicted_rate'] = (tempdf['adjclose_15']-tempdf['adjclose'])/tempdf['adjclose']\n",
    "    tempdf['true_rate'] = (tempdf['true_adjclose_15']-tempdf['adjclose'])/tempdf['adjclose']\n",
    "    seq.append(tempdf[['ticker', 'date', \"predicted_rate\", 'true_rate']])\n",
    "\n",
    "detailstat = pd.concat(seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalstat = detailstat[detailstat['predicted_rate'].between(-2,2) & detailstat['true_rate'].between(-2,2)]\n",
    "largestat = normalstat[~ normalstat['predicted_rate'].between(-0.25,0.25) & ~ normalstat['true_rate'].between(-0.25,0.25)]\n",
    "largestat.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = largestat.plot.scatter(x='predicted_rate', y='true_rate',c='DarkBlue', figsize=(10,10))\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist([normalstat['predicted_rate'],normalstat['predicted_rate']],\n",
    "#          bins=100, range=(-1,1), color = ['r','g'])\n",
    "\n",
    "corr = largestat[[\"predicted_rate\", \"true_rate\"]].corr()\n",
    "\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.regplot(x=largestat[\"predicted_rate\"], y=largestat[\"true_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(normalstat[[\"predicted_rate\", \"true_rate\"]].corr(), annot = True, fmt='.2g',cmap= 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    Event   Cost\n",
      "0  10/2/2011    Music  10000\n",
      "1  11/2/2011   Poetry   5000\n",
      "2  12/2/2011  Theatre  15000\n",
      "3  13/2/2011   Comedy   2000\n",
      "        Date    Event   Cost  Discounted_Price\n",
      "0  10/2/2011    Music  10000            9000.0\n",
      "1  11/2/2011   Poetry   5000            4500.0\n",
      "2  12/2/2011  Theatre  15000           13500.0\n",
      "3  13/2/2011   Comedy   2000            1800.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Date':['10/2/2011', '11/2/2011', '12/2/2011', '13/2/2011'],\n",
    "                    'Event':['Music', 'Poetry', 'Theatre', 'Comedy'],\n",
    "                    'Cost':[10000, 5000, 15000, 2000]})\n",
    " \n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "df['Discounted_Price'] = df.apply(lambda row: row.Cost -\n",
    "                                  (row.Cost * 0.1), axis = 1)\n",
    " \n",
    "# Print the DataFrame after addition\n",
    "# of new column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjlow</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pred_adjclose</th>\n",
       "      <th>true_adjclose</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>132.91</td>\n",
       "      <td>139.13</td>\n",
       "      <td>138.01</td>\n",
       "      <td>138.90</td>\n",
       "      <td>133.80</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>134.44</td>\n",
       "      <td>131.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>130.07</td>\n",
       "      <td>140.49</td>\n",
       "      <td>135.08</td>\n",
       "      <td>136.27</td>\n",
       "      <td>131.26</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>138.69</td>\n",
       "      <td>134.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>128.26</td>\n",
       "      <td>135.68</td>\n",
       "      <td>133.21</td>\n",
       "      <td>134.69</td>\n",
       "      <td>129.74</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.33</td>\n",
       "      <td>131.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>131.40</td>\n",
       "      <td>139.38</td>\n",
       "      <td>136.46</td>\n",
       "      <td>137.71</td>\n",
       "      <td>132.65</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.59</td>\n",
       "      <td>133.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>127.77</td>\n",
       "      <td>134.58</td>\n",
       "      <td>132.25</td>\n",
       "      <td>133.68</td>\n",
       "      <td>129.20</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.39</td>\n",
       "      <td>134.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>131.67</td>\n",
       "      <td>139.20</td>\n",
       "      <td>136.29</td>\n",
       "      <td>137.78</td>\n",
       "      <td>133.16</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.07</td>\n",
       "      <td>132.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>131.85</td>\n",
       "      <td>139.75</td>\n",
       "      <td>136.46</td>\n",
       "      <td>137.52</td>\n",
       "      <td>132.91</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>136.43</td>\n",
       "      <td>134.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>131.82</td>\n",
       "      <td>137.52</td>\n",
       "      <td>136.43</td>\n",
       "      <td>137.39</td>\n",
       "      <td>132.78</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.32</td>\n",
       "      <td>131.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>135.34</td>\n",
       "      <td>142.37</td>\n",
       "      <td>140.07</td>\n",
       "      <td>141.07</td>\n",
       "      <td>136.34</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>136.21</td>\n",
       "      <td>134.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-20</th>\n",
       "      <td>133.57</td>\n",
       "      <td>141.65</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.44</td>\n",
       "      <td>134.76</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.17</td>\n",
       "      <td>135.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            adjlow   high    low  close  adjclose ticker  pred_adjclose  \\\n",
       "date                                                                      \n",
       "2019-07-12  132.91 139.13 138.01 138.90    133.80   MSFT         134.44   \n",
       "2019-07-31  130.07 140.49 135.08 136.27    131.26   MSFT         138.69   \n",
       "2019-08-06  128.26 135.68 133.21 134.69    129.74   MSFT         137.33   \n",
       "2019-08-09  131.40 139.38 136.46 137.71    132.65   MSFT         135.59   \n",
       "2019-08-15  127.77 134.58 132.25 133.68    129.20   MSFT         135.39   \n",
       "2019-08-22  131.67 139.20 136.29 137.78    133.16   MSFT         137.07   \n",
       "2019-09-09  131.85 139.75 136.46 137.52    132.91   MSFT         136.43   \n",
       "2019-09-17  131.82 137.52 136.43 137.39    132.78   MSFT         135.32   \n",
       "2019-09-19  135.34 142.37 140.07 141.07    136.34   MSFT         136.21   \n",
       "2019-09-20  133.57 141.65 138.25 139.44    134.76   MSFT         137.17   \n",
       "\n",
       "            true_adjclose  \n",
       "date                       \n",
       "2019-07-12         131.87  \n",
       "2019-07-31         134.14  \n",
       "2019-08-06         131.19  \n",
       "2019-08-09         133.24  \n",
       "2019-08-15         134.44  \n",
       "2019-08-22         132.71  \n",
       "2019-09-09         134.37  \n",
       "2019-09-17         131.12  \n",
       "2019-09-19         134.44  \n",
       "2019-09-20         135.00  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Timestamp\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'adjlow': {\n",
    "        0: 132.91, 1: 130.07, 2: 128.26, 3: 131.4, 4: 127.77, 5: 131.67, 6: 131.85, 7: 131.82, 8: 135.34, 9: 133.57,},\n",
    "    'high': {\n",
    "        0: 139.13, 1: 140.49, 2: 135.68, 3: 139.38, 4: 134.58,\n",
    "        5: 139.2,\n",
    "        6: 139.75,\n",
    "        7: 137.52,\n",
    "        8: 142.37,\n",
    "        9: 141.65,\n",
    "        },\n",
    "    'low': {\n",
    "        0: 138.01,\n",
    "        1: 135.08,\n",
    "        2: 133.21,\n",
    "        3: 136.46,\n",
    "        4: 132.25,\n",
    "        5: 136.29,\n",
    "        6: 136.46,\n",
    "        7: 136.43,\n",
    "        8: 140.07,\n",
    "        9: 138.25,\n",
    "        },\n",
    "    'close': {\n",
    "        0: 138.9,\n",
    "        1: 136.27,\n",
    "        2: 134.69,\n",
    "        3: 137.71,\n",
    "        4: 133.68,\n",
    "        5: 137.78,\n",
    "        6: 137.52,\n",
    "        7: 137.39,\n",
    "        8: 141.07,\n",
    "        9: 139.44,\n",
    "        },\n",
    "    'adjclose': {\n",
    "        0: 133.8,\n",
    "        1: 131.26,\n",
    "        2: 129.74,\n",
    "        3: 132.65,\n",
    "        4: 129.2,\n",
    "        5: 133.16,\n",
    "        6: 132.91,\n",
    "        7: 132.78,\n",
    "        8: 136.34,\n",
    "        9: 134.76,\n",
    "        },\n",
    "    'ticker': {\n",
    "        0: 'MSFT',\n",
    "        1: 'MSFT',\n",
    "        2: 'MSFT',\n",
    "        3: 'MSFT',\n",
    "        4: 'MSFT',\n",
    "        5: 'MSFT',\n",
    "        6: 'MSFT',\n",
    "        7: 'MSFT',\n",
    "        8: 'MSFT',\n",
    "        9: 'MSFT',\n",
    "        },\n",
    "    'date': {\n",
    "        0: Timestamp('2019-07-12 00:00:00'),\n",
    "        1: Timestamp('2019-07-31 00:00:00'),\n",
    "        2: Timestamp('2019-08-06 00:00:00'),\n",
    "        3: Timestamp('2019-08-09 00:00:00'),\n",
    "        4: Timestamp('2019-08-15 00:00:00'),\n",
    "        5: Timestamp('2019-08-22 00:00:00'),\n",
    "        6: Timestamp('2019-09-09 00:00:00'),\n",
    "        7: Timestamp('2019-09-17 00:00:00'),\n",
    "        8: Timestamp('2019-09-19 00:00:00'),\n",
    "        9: Timestamp('2019-09-20 00:00:00'),\n",
    "        },\n",
    "    'pred_adjclose': {\n",
    "        0: 134.43833923339844,\n",
    "        1: 138.68714904785156,\n",
    "        2: 137.3265380859375,\n",
    "        3: 135.59080505371094,\n",
    "        4: 135.3883514404297,\n",
    "        5: 137.06692504882812,\n",
    "        6: 136.4296417236328,\n",
    "        7: 135.31634521484375,\n",
    "        8: 136.20584106445312,\n",
    "        9: 137.16757202148438,\n",
    "        },\n",
    "    'true_adjclose': {\n",
    "        0: 131.87,\n",
    "        1: 134.14,\n",
    "        2: 131.19,\n",
    "        3: 133.24,\n",
    "        4: 134.44,\n",
    "        5: 132.71,\n",
    "        6: 134.37,\n",
    "        7: 131.12,\n",
    "        8: 134.44,\n",
    "        9: 135.0,\n",
    "        },\n",
    "    })\n",
    "df = df.set_index('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalTrading:\n",
    "    buy_profit  = lambda row : row.true_adjclose - row.adjclose if row.pred_adjclose > row.adjclose else 0\n",
    "    sell_profit = lambda row: row.adjclose - row.true_adjclose if row.pred_adjclose < row.adjclose else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjlow</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pred_adjclose</th>\n",
       "      <th>true_adjclose</th>\n",
       "      <th>buy_profit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-12</th>\n",
       "      <td>132.91</td>\n",
       "      <td>139.13</td>\n",
       "      <td>138.01</td>\n",
       "      <td>138.90</td>\n",
       "      <td>133.80</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>134.44</td>\n",
       "      <td>131.87</td>\n",
       "      <td>-1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>130.07</td>\n",
       "      <td>140.49</td>\n",
       "      <td>135.08</td>\n",
       "      <td>136.27</td>\n",
       "      <td>131.26</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>138.69</td>\n",
       "      <td>134.14</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-06</th>\n",
       "      <td>128.26</td>\n",
       "      <td>135.68</td>\n",
       "      <td>133.21</td>\n",
       "      <td>134.69</td>\n",
       "      <td>129.74</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.33</td>\n",
       "      <td>131.19</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-09</th>\n",
       "      <td>131.40</td>\n",
       "      <td>139.38</td>\n",
       "      <td>136.46</td>\n",
       "      <td>137.71</td>\n",
       "      <td>132.65</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.59</td>\n",
       "      <td>133.24</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>127.77</td>\n",
       "      <td>134.58</td>\n",
       "      <td>132.25</td>\n",
       "      <td>133.68</td>\n",
       "      <td>129.20</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.39</td>\n",
       "      <td>134.44</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-22</th>\n",
       "      <td>131.67</td>\n",
       "      <td>139.20</td>\n",
       "      <td>136.29</td>\n",
       "      <td>137.78</td>\n",
       "      <td>133.16</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.07</td>\n",
       "      <td>132.71</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-09</th>\n",
       "      <td>131.85</td>\n",
       "      <td>139.75</td>\n",
       "      <td>136.46</td>\n",
       "      <td>137.52</td>\n",
       "      <td>132.91</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>136.43</td>\n",
       "      <td>134.37</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-17</th>\n",
       "      <td>131.82</td>\n",
       "      <td>137.52</td>\n",
       "      <td>136.43</td>\n",
       "      <td>137.39</td>\n",
       "      <td>132.78</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>135.32</td>\n",
       "      <td>131.12</td>\n",
       "      <td>-1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-19</th>\n",
       "      <td>135.34</td>\n",
       "      <td>142.37</td>\n",
       "      <td>140.07</td>\n",
       "      <td>141.07</td>\n",
       "      <td>136.34</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>136.21</td>\n",
       "      <td>134.44</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-20</th>\n",
       "      <td>133.57</td>\n",
       "      <td>141.65</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.44</td>\n",
       "      <td>134.76</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>137.17</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            adjlow   high    low  close  adjclose ticker  pred_adjclose  \\\n",
       "date                                                                      \n",
       "2019-07-12  132.91 139.13 138.01 138.90    133.80   MSFT         134.44   \n",
       "2019-07-31  130.07 140.49 135.08 136.27    131.26   MSFT         138.69   \n",
       "2019-08-06  128.26 135.68 133.21 134.69    129.74   MSFT         137.33   \n",
       "2019-08-09  131.40 139.38 136.46 137.71    132.65   MSFT         135.59   \n",
       "2019-08-15  127.77 134.58 132.25 133.68    129.20   MSFT         135.39   \n",
       "2019-08-22  131.67 139.20 136.29 137.78    133.16   MSFT         137.07   \n",
       "2019-09-09  131.85 139.75 136.46 137.52    132.91   MSFT         136.43   \n",
       "2019-09-17  131.82 137.52 136.43 137.39    132.78   MSFT         135.32   \n",
       "2019-09-19  135.34 142.37 140.07 141.07    136.34   MSFT         136.21   \n",
       "2019-09-20  133.57 141.65 138.25 139.44    134.76   MSFT         137.17   \n",
       "\n",
       "            true_adjclose  buy_profit  \n",
       "date                                   \n",
       "2019-07-12         131.87       -1.93  \n",
       "2019-07-31         134.14        2.88  \n",
       "2019-08-06         131.19        1.45  \n",
       "2019-08-09         133.24        0.59  \n",
       "2019-08-15         134.44        5.24  \n",
       "2019-08-22         132.71       -0.45  \n",
       "2019-09-09         134.37        1.46  \n",
       "2019-09-17         131.12       -1.66  \n",
       "2019-09-19         134.44        0.00  \n",
       "2019-09-20         135.00        0.24  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trade = NormalTrading\n",
    "df[\"buy_profit\"] = list(df.apply(trade.buy_profit, axis=1)\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "StockML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
